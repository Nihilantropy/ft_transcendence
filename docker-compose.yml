services:
  ### NGINX ###
  nginx:
    container_name: ft_transcendence_nginx
    image: ft_transcendence_nginx:local
    build:
      context: ./srcs/nginx
      dockerfile: Dockerfile
    env_file:
      - ./srcs/nginx/.env
    ports:
      - "80:80"
      - "443:443"
    networks:
      - proxy           # External access
      - backend-network # To reach API Gateway (production-like)
    restart: on-failure
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  ### OLLAMA ###
  ollama:
    container_name: ollama
    image: ollama/ollama:latest
    runtime: nvidia
    entrypoint: ["/workspace/init.sh"]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - LOG_LEVEL=info
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all
    volumes:
      - ollama:/root/.ollama
      - models:/models
      - ./srcs/ollama/:/workspace/:rw
    ports:
      - "11434:11434"
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "2"
    networks:
      - backend-network
    restart: unless-stopped

  ai-service:
    build:
      context: ./srcs/ai
      dockerfile: Dockerfile
    container_name: ft_transcendence_ai_service
    env_file:
      - ./srcs/ai/.env
    networks:
      - backend-network
    volumes:
      - ./srcs/ai/src:/app/src
      - ./srcs/ai/tests:/app/tests
      - ./srcs/ai/data/knowledge_base:/app/data/knowledge_base:ro
      - ai-chroma-data:/app/data/chroma
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
### CLASSIFICATION SERVICE ###
  classification-service:
    build:
      context: ./srcs/classification-service
      dockerfile: Dockerfile
    container_name: ft_transcendence_classification_service
    env_file:
      - ./srcs/classification-service/.env
    # GPU enabled - RTX 5060 Ti (Blackwell) support via PyTorch 2.11 nightly + CUDA 12.8
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      # DEVICE controlled via .env file (auto/cuda/cpu)
      - TRANSFORMERS_CACHE=/app/.cache/huggingface
      - HF_HOME=/app/.cache/huggingface
    volumes:
      - huggingface-cache:/app/.cache/huggingface
    networks:
      - backend-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  # open-webui:
  #   container_name: open-webui
  #   image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG-main}
  #   environment:
  #     - MODEL_DOWNLOAD_DIR=/models
  #     - OLLAMA_BASE_URL=http://ollama:11434
  #     - WEBUI_SECRET_KEY=your_secret_key_here  # Add this to prevent logouts after updates
  #   volumes:
  #     - open-webui:/app/backend/data
  #   ports:
  #     - ${OPEN_WEBUI_PORT-3000}:8080
  #   logging:
  #     driver: json-file
  #     options:
  #       max-size: "5m"
  #       max-file: "2"
  #   depends_on:
  #     - ollama
  #   extra_hosts:
  #     - "host.docker.internal:host-gateway"
  #   networks:
  #     - backend-network
  #   restart: unless-stopped
### FRONTEND ###
  # frontend:
  #   container_name: ft_transcendence_frontend
  #   image: ft_transcendence_frontend:local
  #   build:
  #     context: ./srcs/frontend
  #     dockerfile: Dockerfile
  #   env_file:
  #     - ./srcs/frontend/.env
  #   volumes:
  #     # Mount source code for development hot reload
  #     - ./srcs/frontend/src:/app/src:rw
  #     - ./srcs/frontend/public:/app/public:rw
  #   networks:
  #     - proxy
  #   restart: on-failure
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:5173"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 30s
### AUTH SERVICE ###
  auth-service:
    container_name: ft_transcendence_auth_service
    image: ft_transcendence_auth_service:local
    build:
      context: ./srcs/auth-service
      dockerfile: Dockerfile
    env_file:
      - ./srcs/auth-service/.env
    volumes:
      # Mount source code for development hot reload
      - ./srcs/auth-service:/app:rw
    networks:
      - backend-network
    restart: on-failure
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3001/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    depends_on:
      db:
        condition: service_healthy
### USER SERVICE ###
  user-service:
    container_name: ft_transcendence_user_service
    image: ft_transcendence_user_service:local
    build:
      context: ./srcs/user-service
      dockerfile: Dockerfile
    env_file:
      - ./srcs/user-service/.env
    volumes:
      # Mount source code for development hot reload
      - ./srcs/user-service:/app:rw
    networks:
      - backend-network
    restart: on-failure
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3002/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    depends_on:
      db:
        condition: service_healthy
### REDIS ###
  redis:
    container_name: ft_transcendence_redis
    image: redis:8.4.0-alpine3.22
    command: redis-server --appendonly yes
    volumes:
      - redis-data:/data
    networks:
      - backend-network
    restart: on-failure
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "2"
### DATABASE ###
  db:
    container_name: ft_transcendence_db
    image: postgres:15-alpine
    env_file:
      - ./srcs/db/.env
    volumes:
      - db-data:/var/lib/postgresql/data
      - ./srcs/db/init-scripts:/docker-entrypoint-initdb.d:ro
    networks:
      - backend-network
    restart: on-failure
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER-smartbreeds_user} -d ${POSTGRES_DB-smartbreeds}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "2"
### PGADMIN ###
  pgadmin:
    container_name: ft_transcendence_pgadmin
    image: dpage/pgadmin4:9.11.0
    ports:
      - "8080:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: smartbreeds_user@email.com
      PGADMIN_DEFAULT_PASSWORD: smartbreeds_password
      PGADMIN_SERVER_JSON_FILE: "/pgadmin4/servers.json"
    volumes:
      - pgadmin-data:/var/lib/pgadmin
      - ./srcs/db/config/servers.json:/pgadmin4/servers.json:ro
    networks:
      - backend-network
    restart: on-failure
### API GATEWAY ###
  api-gateway:
    container_name: ft_transcendence_api_gateway
    image: ft_transcendence_api_gateway:local
    build:
      context: ./srcs/api-gateway
      dockerfile: Dockerfile
    env_file:
      - ./srcs/api-gateway/.env
    ports:
      - "8001:8001"  # Expose for development testing (curl/Postman)
    volumes:
      # Mount source code for development hot reload
      - ./srcs/api-gateway/src:/app/src:rw
      - ./srcs/api-gateway/routes:/app/routes:rw
      # Mount tests for development
      - ./srcs/api-gateway/tests:/app/tests:rw
      # Share JWT public key from auth-service (RS256 verification)
      - ./srcs/auth-service/keys/jwt-public.pem:/app/keys/jwt-public.pem:ro
    networks:
      - backend-network  # Internal only (production-like), NGINX proxies to it
    restart: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    depends_on:
      auth-service:
        condition: service_healthy
      user-service:
        condition: service_healthy

### RECOMMENDATION SERVICE ###
  recommendation-service:
    build:
      context: ./srcs/recommendation-service
      dockerfile: Dockerfile
    container_name: ft_transcendence_recommendation_service
    restart: unless-stopped
    networks:
      - backend-network
    depends_on:
      - user-service
    env_file:
      - ./srcs/recommendation-service/.env
    volumes:
      - ./srcs/recommendation-service:/app
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3005/health"]
      interval: 30s
      timeout: 10s
      retries: 3

### NETWORKS ###
networks:
  proxy:
    driver: bridge
  backend-network:
    driver: bridge

### VOLUMES ###
volumes:
  frontend-data:
    driver: local
  db-data:
    driver: local
  pgadmin-data:
    driver: local
  redis-data:
    driver: local
  ollama:
    driver: local
  models:
    driver: local
  open-webui:
    driver: local
  ai-chroma-data:
    driver: local
  huggingface-cache:
    driver: local
